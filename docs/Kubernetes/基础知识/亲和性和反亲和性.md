### 核心思想：一句话理解亲和与反亲和



想象一下你要安排一些人（Pods）住进不同的房子（Nodes）。

- **亲和性 (Affinity)**：就是“**我想和谁住得近一些**”。比如，“A”这个人希望能和“B”这个人住在同一栋房子里，或者希望住进有“游泳池”标签的房子。
- **反亲和性 (Anti-Affinity)**：就是“**我不想和谁住在一起**”。比如，“C”这个人和“D”这个人关系不好，你必须把他们安排在不同的房子里，以避免冲突。或者为了安全，你不能把所有“保安”都安排在同一栋房子里，万一这栋房子出事了，整个安保系统就瘫痪了。

在 Kubernetes 中，这种“远近关系”的规则就是通过亲和性与反亲和性来定义的，它指导 Kube-scheduler 如何为新创建的 Pod 选择最合适的节点。

------



### 两大分类：节点亲和性 vs Pod 亲和性



亲和性与反亲和性主要分为两大类，区别在于**评判的依据是节点（Node）还是已经运行的 Pod**。

1. **节点亲和性 (Node Affinity)**：依据**节点的标签 (Labels)** 来决定 Pod 是否可以被调度到该节点上。
   - **场景**：当你的应用需要特定硬件（如 GPU、SSD 硬盘）或需要被部署到特定的可用区（Availability Zone）时，这个功能非常有用。
2. **Pod 亲和性与反亲和性 (Pod Affinity and Anti-Affinity)**：依据**其他 Pod 的标签 (Labels)** 来决定新 Pod 的调度位置。
   - **场景**：
     - **Pod 亲和性**：为了降低网络延迟，你希望将前端 Web 服务的 Pod 和后端缓存（如 Redis）的 Pod 部署在同一个节点上。
     - **Pod 反亲和性**：为了实现高可用，你希望将一个应用（如 MySQL 数据库）的多个副本分散部署在不同的节点或机架上，避免单点故障。

------



### 两种策略：硬策略 vs 软策略



无论是节点亲和性还是 Pod 亲和性/反亲和性，它们都有两种执行策略：

1. **硬策略 (Required / 必须满足)**：规则**必须**被满足，否则 Pod 将无法被调度，会一直处于 `Pending` 状态。
   - 在 YAML 中对应的字段是 `requiredDuringSchedulingIgnoredDuringExecution`。
2. **软策略 (Preferred / 倾向满足)**：调度器会**尽量**满足规则，但如果无法满足，Pod 仍然会被调度到某个不符合规则的节点上。这是一种“尽力而为”的模式。
   - 在 YAML 中对应的字段是 `preferredDuringSchedulingIgnoredDuringExecution`。

IgnoredDuringExecution 的含义：

这个后缀的意思是“在执行期间忽略”。也就是说，亲和性规则仅在**调度期间（Scheduling）**生效。一旦 Pod 成功调度并运行在某个节点上，即使后来该节点的标签发生变化（对于 Node Affinity）或者相关 Pod 状态变化（对于 Pod Affinity）导致不再满足亲和性规则，该 Pod 不会被驱逐，会继续在该节点上运行。

------



### 一、节点亲和性 (Node Affinity) 详解



这是最简单的一种。它取代了旧的 `nodeSelector`，功能更强大。



#### 示例 1：硬策略 (Required)



**需求**：我的应用需要 GPU 进行计算，必须被调度到带有 `gpu=true` 标签的节点上。

首先，给节点打上标签：

Bash

```
kubectl label nodes <your-node-name> gpu=true
```

然后，在 Pod 的 YAML 文件中定义 `nodeAffinity`：

YAML

```
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  containers:
  - name: my-container
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu
            operator: In
            values:
            - "true"
```

**解读**：

- `requiredDuringSchedulingIgnoredDuringExecution`：表明这是一个硬性要求。
- `matchExpressions`：定义了匹配规则。
  - `key: gpu`：匹配标签的 key。
  - `operator: In`：操作符，表示 key 的值在 `values` 列表中。
  - `values: ["true"]`：标签的值。

如果集群中没有任何一个节点的标签是 `gpu=true`，这个 Pod 将永远是 `Pending` 状态。



#### 示例 2：软策略 (Preferred)



**需求**：我希望我的应用优先被调度到拥有 SSD 硬盘的节点（标签为 `disktype=ssd`），但如果没有这样的节点，调度到其他节点上也行。

给节点打标签：

Bash

```
kubectl label nodes <your-node-name> disktype=ssd
```

Pod 的 YAML 定义：

YAML

```
apiVersion: v1
kind: Pod
metadata:
  name: ssd-preferred-pod
spec:
  containers:
  - name: my-container
    image: nginx
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1 # 权重值，范围 1-100
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
```

**解读**：

- `preferredDuringSchedulingIgnoredDuringExecution`：表明这是一个软性要求。
- `weight`：权重。调度器会计算所有候选节点的分数，分数越高的节点越优先。满足这个偏好条件的节点会获得 `weight` 值的加分。你可以定义多条偏好规则，权重越高的规则越重要。

------



### 二、Pod 亲和性与反亲和性 (Pod Affinity & Anti-Affinity) 详解



这是更复杂但非常有用的部分。它依赖于一个关键概念：**`topologyKey`**。

topologyKey 是什么？

它定义了一个“区域”或“范围”的概念。亲和/反亲和规则就在这个 topologyKey 定义的范围内生效。topologyKey 的值是节点的标签 key。

- **`kubernetes.io/hostname`**：这是最常用的，它将“区域”定义为**单个节点**。规则的含义是“在同一个节点上”或者“不要在同一个节点上”。
- **`topology.kubernetes.io/zone`**：将“区域”定义为**同一个可用区**。
- **`topology.kubernetes.io/region`**：将“区域”定义为**同一个区域**。



#### 示例 3：Pod 亲和性 (硬策略)



**需求**：为了低延迟，我希望将我的 `frontend-pod` 调度到已经运行着 `backend-cache` Pod 的同一个节点上。

假设 `backend-cache` Pod 已经带有了 `app=backend-cache` 的标签。

YAML

```
apiVersion: v1
kind: Pod
metadata:
  name: frontend-pod
spec:
  containers:
  - name: my-frontend
    image: nginx
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - backend-cache
        topologyKey: "kubernetes.io/hostname"
```

**解读**：

- `podAffinity`：定义 Pod 亲和性规则。
- `labelSelector`：用于查找目标 Pod（这里是 `app=backend-cache`）。
- `topologyKey: "kubernetes.io/hostname"`：这条规则的生效范围是单个节点。
- **整体含义**：调度器在为 `frontend-pod` 选择节点时，必须选择一个已经运行了至少一个带有 `app=backend-cache` 标签的 Pod 的节点。



#### 示例 4：Pod 反亲和性 (硬策略) - 高可用经典用法



**需求**：我有一个高可用的数据库应用（`app=mariadb`），它有3个副本。为了防止单点故障，我**绝对不希望**任何两个副本运行在同一个节点上。

YAML

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mariadb
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mariadb
  template:
    metadata:
      labels:
        app: mariadb
    spec:
      containers:
      - name: mariadb
        image: mariadb
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - mariadb
            topologyKey: "kubernetes.io/hostname"
```

**解读**：

- `podAntiAffinity`：定义 Pod 反亲和性规则。
- `labelSelector`：匹配的目标是带有 `app=mariadb` 标签的 Pod（也就是它自己所在 Deployment 的其他副本）。
- `topologyKey: "kubernetes.io/hostname"`：生效范围是单个节点。
- **整体含义**：在调度一个新的 `mariadb` Pod 时，调度器**不能**选择一个已经运行着 `app=mariadb` Pod 的节点。这强制性地将3个副本分散到3个不同的节点上，实现了节点级别的高可用。



#### 示例 5：Pod 反亲和性 (软策略)



**需求**：我希望我的 `web-server` 应用的 Pod 能够**尽量**分散在不同的可用区（zone），但如果做不到，部署在同一个区也可以接受。

YAML

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  replicas: 5
  selector:
    matchLabels:
      app: web-server
  template:
    metadata:
      labels:
        app: web-server
    spec:
      containers:
      - name: web
        image: nginx
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - web-server
              topologyKey: "topology.kubernetes.io/zone"
```

**解读**：

- `preferredDuringSchedulingIgnoredDuringExecution`: 软策略。
- `weight: 100`: 给这个规则一个很高的权重。
- `topologyKey: "topology.kubernetes.io/zone"`：生效范围是可用区。
- **整体含义**：调度器会**优先**选择一个还没有 `web-server` Pod 的可用区中的节点。如果所有可用区都已经有了 `web-server` Pod，调度器会选择一个 `web-server` Pod 数量最少的可用区。

------



### 总结与对比



| 类型                  | 依据                | 目的                                            | 常见用例                                                     |
| --------------------- | ------------------- | ----------------------------------------------- | ------------------------------------------------------------ |
| **Node Affinity**     | **节点的标签**      | 将 Pod 调度到**特定类型**的节点上               | - 需要 GPU/SSD 的应用 - 将 Pod 限制在特定可用区              |
| **Pod Affinity**      | **其他 Pod 的标签** | 将**关联的 Pod** 调度到一起                     | - 前端和后端服务，降低网络延迟 - 应用和日志收集器            |
| **Pod Anti-Affinity** | **其他 Pod 的标签** | 将**互斥的 Pod** 或**同一应用的多个副本**分散开 | - **实现高可用 (最重要的用例！)** - 避免有资源竞争关系的 Pod 互相影响 |



### 最佳实践与注意事项



1. **优先使用软策略 (Preferred)**：除非是“没有它就不能运行”的硬性要求（如必须有 GPU），否则尽量使用软策略。硬策略可能导致你的 Pod 因为找不到合适的节点而永远无法调度。
2. **Pod 反亲和性是高可用的基石**：对于任何需要高可用的有状态或无状态应用，`required` 级别的 Pod 反亲和性（配合 `topologyKey: kubernetes.io/hostname`）几乎是标准配置。
3. **注意 `topologyKey` 的选择**：根据你的高可用级别选择合适的 `topologyKey`。`hostname` 是节点级，`zone` 是可用区级。
4. **标签管理很重要**：亲和性与反亲和性完全依赖于标签。保持标签的清晰、一致和规范化是成功运用这些功能的关键。
5. **性能考虑**：Pod 亲和性/反亲和性会增加调度器的计算负担，因为它需要评估集群中所有 Pod 的标签。在超大规模集群中（成千上万个节点），应谨慎使用。

希望这份详细的说明能够帮助你更好地理解和运用 Kubernetes 的亲和性与反亲和性功能！